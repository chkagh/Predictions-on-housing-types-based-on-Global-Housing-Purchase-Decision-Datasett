# -*- coding: utf-8 -*-
"""Python in ML - Lab 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XWiEU_xcUJOVhajSOwQ1NbRFdGEBRcfh
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1) DATA DIVISION & VISUALIZATIONS

## Subsets division
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split


data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Lab5/global_house_purchase_dataset.csv')



TARGET = 'property_type'

X = data.drop(TARGET, axis=1)
y = data[TARGET]

# Spliting to train data and temp data which will be splited to validation and test set
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Spliting to test data and valid data
X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, train_size=0.7, random_state=1)

data_train = pd.concat([X_train, y_train], axis=1)
data_valid = pd.concat([X_valid, y_valid], axis=1)
data_test = pd.concat([X_test, y_test], axis=1)

"""## Plots"""

correlation_matrix = data_train.select_dtypes(include=['number']).corr()

plt.figure(figsize=(25, 6))

sns.heatmap(correlation_matrix, annot=True, cmap='PiYG', fmt=".3f", linewidths=.5)
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

data.head()

print(data.columns)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

columns = data_train.columns
num_columns = len(columns)

num_figures = 5
columns_per_figure = int(np.ceil(num_columns / num_figures))

for i in range(num_figures):
    start_idx = i * columns_per_figure
    end_idx = min((i + 1) * columns_per_figure, num_columns)
    current_columns = columns[start_idx:end_idx]

    if len(current_columns) == 0:
        continue

    num_rows = int(np.ceil(len(current_columns) / 3))
    num_cols = min(len(current_columns), 3)

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows))
    fig.suptitle(f'Histograms for Features (Figure {i+1})', fontsize=16)
    axes = axes.flatten()

    for j, col in enumerate(current_columns):
        ax = axes[j]
        if data_train[col].dtype == 'object':
            sns.countplot(y=data_train[col], ax=ax, order=data_train[col].value_counts().index)
            ax.set_title(f'Distribution of {col}', fontsize=12)
            ax.set_xlabel('Count')
            ax.set_ylabel(col)
        else:
            sns.histplot(data_train[col], ax=ax)
            ax.set_title(f'Distribution of {col}', fontsize=12)
            ax.set_xlabel(col)
            ax.set_ylabel('Frequency')

        if data_train[col].dtype != 'object' and len(data_train[col].unique()) > 10:
            ax.tick_params(axis='x', rotation=45)

    for k in range(j + 1, len(axes)):
        fig.delaxes(axes[k])

    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    plt.show()

"""# 2) DATA PREPARATION

## Data Inspection
"""

data.info()

data.describe()

data.head()

"""## Unique values"""

data.nunique()

"""## Missing data"""

data.isnull().sum()

"""## Features Engineering"""

# import datetime

# # Calculate property_age
# current_year = datetime.datetime.now().year
# X_train['property_age'] = current_year - X_train['constructed_year']
# X_valid['property_age'] = current_year - X_valid['constructed_year']
# X_test['property_age'] = current_year - X_test['constructed_year']

# # Calculate price_per_sqft
# X_train['price_per_sqft'] = X_train['price'] / X_train['property_size_sqft']
# X_valid['price_per_sqft'] = X_valid['price'] / X_valid['property_size_sqft']
# X_test['price_per_sqft'] = X_test['price'] / X_test['property_size_sqft']

# X_train.info()
# X_train.head()

"""## Categorical Feature Encoding

### Removing 'Unnamed:0' & 'property_id' columns

They can cause noise
"""

X_train = X_train.drop(['property_id'], axis=1)
X_valid = X_valid.drop(['property_id'], axis=1)
X_test = X_test.drop(['property_id'], axis=1)
data_train = data_train.drop(['property_id'], axis=1)
data_valid = data_valid.drop(['property_id'], axis=1)
data_test = data_test.drop(['property_id'], axis=1)

"""### Identifying nominal features (One-Hot_ Encoding)
* country
* city
"""

X_train.head()

# from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
# import pandas as pd

# nominal_features = ['country', 'city']

# one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# # Fit on X_train and transform all three datasets
# X_train_nominal_encoded = one_hot_encoder.fit_transform(X_train[nominal_features])
# X_valid_nominal_encoded = one_hot_encoder.transform(X_valid[nominal_features])
# X_test_nominal_encoded = one_hot_encoder.transform(X_test[nominal_features])

# # Create DataFrames for encoded nominal features with proper column names
# X_train_nominal_df = pd.DataFrame(X_train_nominal_encoded, columns=one_hot_encoder.get_feature_names_out(nominal_features), index=X_train.index)
# X_valid_nominal_df = pd.DataFrame(X_valid_nominal_encoded, columns=one_hot_encoder.get_feature_names_out(nominal_features), index=X_valid.index)
# X_test_nominal_df = pd.DataFrame(X_test_nominal_encoded, columns=one_hot_encoder.get_feature_names_out(nominal_features), index=X_test.index)

# X_train_nominal_df.info()

# X_train_nominal_df.head()

"""### Identifying ordinal features (Ordinal Encoder)
* furnishing_status

"""

from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
import pandas as pd

#ordinal_features = ['furnishing_status']
ordinal_features = ['furnishing_status', 'garden', 'loan_tenure_years', 'satisfaction_score', 'neighbourhood_rating', 'connectivity_score']

furnishing_order = ['Unfurnished', 'Semi-Furnished', 'Fully-Furnished']

#ordinal_encoder = OrdinalEncoder(categories=[furnishing_order])
ordinal_encoder = OrdinalEncoder(categories=[furnishing_order, sorted(X_train['garden'].unique()),
                                            sorted(X_train['loan_tenure_years'].unique()),
                                            sorted(X_train['satisfaction_score'].unique()),
                                            sorted(X_train['neighbourhood_rating'].unique()),
                                            sorted(X_train['connectivity_score'].unique())])


X_train[ordinal_features] = ordinal_encoder.fit_transform(X_train[ordinal_features])
X_valid[ordinal_features] = ordinal_encoder.transform(X_valid[ordinal_features])
X_test[ordinal_features] = ordinal_encoder.transform(X_test[ordinal_features])

unique_cities = data['city'].unique()
print(f"There are {len(unique_cities)} unique cities.\n")
print(unique_cities)

X_train.head()

X_train = X_train.drop(['country', 'city'], axis=1)
X_valid = X_valid.drop(['country', 'city'], axis=1)
X_test = X_test.drop(['country', 'city'], axis=1)

# X_train = pd.concat([X_train.drop(columns=nominal_features), X_train_nominal_df], axis=1)
# X_valid = pd.concat([X_valid.drop(columns=nominal_features), X_valid_nominal_df], axis=1)
# X_test = pd.concat([X_test.drop(columns=nominal_features), X_test_nominal_df], axis=1)



X_train.info()



"""# Applying model"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

clf = Pipeline([("scaler", StandardScaler()), ("log", LogisticRegression(max_iter = 100000, random_state=1, solver='newton-cg'))])
clf.fit(X_train, y_train)

y_pred_train = clf.predict(X_train)
y_pred = clf.predict(X_valid)

from sklearn.metrics import accuracy_score
acc_train = accuracy_score(y_train, y_pred_train)
acc_val = accuracy_score(y_valid, y_pred)

print(f'Training Accuracy: {acc_train*100:.4f} %')
print(f'Validation Accuracy: {acc_val*100:.4f} %')

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

y_pred = evaluateCLF(clf, X_train, y_train, 'train')
ConfusionMatrixDisplay.from_predictions(y_train, y_pred)
plt.xticks(rotation=45)
plt.show()

"""Training Accuracy: 18.1888 %
Validation Accuracy: 16.6950 %
"""

def evaluateCLF(clf, x, y_target, name = ''):
  pred = clf.predict(x)
  pred_accuracy = accuracy_score(y_target, pred)
  print(name + ' accuracy: ' + str(pred_accuracy))
  return pred

X_train1 = X_train.drop(['down_payment'], axis=1)
X_valid1 = X_valid.drop(['down_payment'], axis=1)
X_test1 = X_test.drop(['down_payment'], axis=1)

clf = Pipeline([("scaler", StandardScaler()), ("log", LogisticRegression(max_iter = 100000, random_state=1, solver='newton-cg'))])
# hiddenLayers = (25, 12)
# maxIterations = 500
#clf = Pipeline([("scaler", StandardScaler()), ("MLP", MLPClassifier( activation='logistic', hidden_layer_sizes=hiddenLayers, max_iter=maxIterations))])
clf.fit(X_train1, y_train)

y_pred_train1 = clf.predict(X_train1)
y_pred1 = clf.predict(X_valid1)

from sklearn.metrics import accuracy_score
acc_train = accuracy_score(y_train, y_pred_train1)
acc_val = accuracy_score(y_valid, y_pred1)

print(f'Training Accuracy: {acc_train*100:.4f} %')
print(f'Validation Accuracy: {acc_val*100:.4f} %')

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

#y_pred = evaluateCLF(clf, X_train, y_train, 'train')
#ConfusionMatrixDisplay.from_predictions(y_train, y_pred)
#plt.xticks(rotation=45)
#plt.show()

y_pred = evaluateCLF(clf, X_train1, y_train, 'train')

ConfusionMatrixDisplay.from_predictions(y_train, y_pred)
plt.xticks(rotation=45)
plt.show()